{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L05qs.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GhyeIVDosAzy","colab_type":"text"},"source":["In this lesson, we trained a linear neural network to distinguish background green pixels from other foreground pixels. Do you think the network will converge to the same weights every time it is run? Try it and see. Is there any property of the weights that seems consistent from one run to another?"]},{"cell_type":"code","metadata":{"id":"8PFoSOH4XZiv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1593171580028,"user_tz":240,"elapsed":1871,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"10ba23b2-59f7-48f5-fda3-0797ffb95a05"},"source":["!wget https://github.com/mlittmancs/great_courses_ml/raw/master/greenML.png"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-06-26 11:39:38--  https://github.com/mlittmancs/great_courses_ml/raw/master/greenML.png\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/mlittmancs/great_courses_ml/master/greenML.png [following]\n","--2020-06-26 11:39:39--  https://raw.githubusercontent.com/mlittmancs/great_courses_ml/master/greenML.png\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1190198 (1.1M) [image/png]\n","Saving to: ‘greenML.png’\n","\n","greenML.png         100%[===================>]   1.13M  --.-KB/s    in 0.09s   \n","\n","2020-06-26 11:39:39 (12.8 MB/s) - ‘greenML.png’ saved [1190198/1190198]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OmxJz39WsEDO","colab_type":"text"},"source":["Here, we download the image."]},{"cell_type":"code","metadata":{"id":"mLDbTVcmDZFl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593171582989,"user_tz":240,"elapsed":2112,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"7360a142-c95e-4e1f-95ef-55580fbbcd60"},"source":["import numpy as np\n","from keras.preprocessing import image\n","\n","img = image.load_img(\"greenML.png\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Jx5w3UgAE2cS","colab_type":"text"},"source":["Below we trim the edges of the image."]},{"cell_type":"code","metadata":{"id":"O5ny7M6UflUb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593171582990,"user_tz":240,"elapsed":284,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["arr = image.img_to_array(img)\n","# Trim off edges\n","arr = arr[:697,:]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3ShPb7xE7Rj","colab_type":"text"},"source":["In this example, we isolate out the bacground and convert it to a dataset of positive examples, `yesList`."]},{"cell_type":"code","metadata":{"id":"yrdg39l6GMtH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593171584969,"user_tz":240,"elapsed":580,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# background\n","tmp = arr[:,:360]\n","\n","yesList = np.reshape(tmp,(-1,3))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_eHUJXK6HVwh","colab_type":"text"},"source":["Below we print the number of values in each dimension of `tmp`."]},{"cell_type":"markdown","metadata":{"id":"zbLvsTBgKnC9","colab_type":"text"},"source":["Now we'll isolate out the foreground and make a dataset of foreground pixels called `noList`."]},{"cell_type":"code","metadata":{"id":"qcSqKYtDwqsq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593171586426,"user_tz":240,"elapsed":383,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# foreground\n","tmp = arr[30:,547:620]\n","\n","noList = np.reshape(tmp,(-1,3))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mluPsOFNL_i8","colab_type":"text"},"source":["We finalize our dataset here, with a variable `alldat` which contains our list of pixels, and `labs` which is our list of labels for each pixel, `0` for green background pixels and `1` for foreground pixels"]},{"cell_type":"code","metadata":{"id":"mfPVKGHoenU2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593171587850,"user_tz":240,"elapsed":375,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# Build a list of pixels for both positive and negative examples.\n","alldat = np.concatenate((yesList,noList))\n"," \n","# labels\n","labs = np.concatenate((np.ones(len(yesList)), np.zeros(len(noList))))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlYPmhdkMDIQ","colab_type":"text"},"source":["Here, we'll build a classifier to separate the background from the foreground\n","\n","We define a `loss` function.  The `loss` takes in our data, `alldat`, our labels `labs`, and our current weights, `w`.  We will make adjustments to `w` based on this loss function.  To make a prediction, we will multiply `w` by `alldat`, and pass it through a sigmoid function to get our predictions, `y`.  We will then compare our predictions `y` to their true labels `labs` using a squared loss, the sum of the squared difference between `labs`, our true labels, and `y` our predicted values"]},{"cell_type":"code","metadata":{"id":"z03-JVB_eqHJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593171593141,"user_tz":240,"elapsed":382,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}}},"source":["# Add an additional column to the data corresponding to \n","#  the offset parameter.\n","alldat = np.concatenate((alldat,np.ones((len(alldat),1))),1)\n","\n","# Compute the loss of the rule specified by weights w with respect\n","#  to the data alldat labeled with labs\n","def loss(w, alldat, labs):\n","  # Compute a weighted sum for each instance\n","  h = np.matmul(alldat,w)\n","  # transform the sum using the sigmoid function\n","  y = 1/(1 + np.exp(-h))\n","  # take the difference between the labels and the output of the \n","  #  sigmoid, squared, then sum up over all instances to get the \n","  #  total loss.\n","  loss = np.sum((labs - y)**2)\n","  return(loss)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-vkgAKUMNFC","colab_type":"text"},"source":["Now we'll train the model, by creating a `fit` function to fit the model to the data.  We will update the weights through gradient descent. "]},{"cell_type":"code","metadata":{"id":"l44hJI6tXvox","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593180680001,"user_tz":240,"elapsed":772768,"user":{"displayName":"Michael Littman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh1OI1zp1NevMSZ87UN47k8bTZ--Ijr4v3ORV33Vg=s64","userId":"06751891446459829367"}},"outputId":"aeb48350-ff76-4e95-e323-75badda700b0"},"source":["def fit(w,alldat,labs):\n","  # alpha represents how big of a step we’ll\n","  #  be taking in the direction of the derivative.\n","  #  It’s called the learning rate.\n","  alpha = 0.1\n","\n","  # We'll stop searching when we're at a (near) local min\n","  done = False\n","  while not done:\n","    # Every 100 iterations or so, let’s\n","    #  take a peek at the weights, the learning\n","    #  rate, and the current loss\n","    if np.random.random() < 0.01: print(w, alpha, loss(w,alldat,labs))\n","    # The next few lines compute the gradient\n","    #  of the loss function. The details aren’t\n","    #  important right now.\n","    # delta_w is the change in the weights\n","    #  suggested by the gradient\n","    h = np.matmul(alldat,w)\n","    y = 1/(1 + np.exp(-h))\n","    delta_w = np.add.reduce(np.reshape((labs-y) * np.exp(-h)*y**2,(len(y),1)) * alldat)\n","    # if we take a step of size alpha and update\n","    #  the weights, we’ll get new weights neww.\n","    current_loss = loss(w,alldat,labs)\n","    alpha *= 2\n","    neww = w + alpha* delta_w\n","    while loss(neww,alldat,labs) >= current_loss and not done:\n","      alpha /= 2\n","      if alpha*max(abs(delta_w)) < 0.0001: \n","        done = True\n","        print(alpha,delta_w)\n","      else: neww = w + alpha* delta_w\n","    if not done: w = neww\n","  return(w)\n","\n","w = np.random.random(4)\n","w = fit(w,alldat,labs)\n","print(w)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["[-0.71973051  0.14904301 -0.4152915  -0.329474  ] 1.220703125e-05 451.06624660571356\n","[-0.72051788  0.14882564 -0.41469419 -0.33076522] 1.220703125e-05 450.99299287332735\n","[-0.72956408  0.1463699  -0.40796775 -0.34585906] 1.220703125e-05 450.2132544185683\n","[-0.73403682  0.14551224 -0.4046298  -0.35349958] 1.220703125e-05 449.8241596211096\n","[-0.74142769  0.14441659 -0.3991371  -0.36639042] 2.44140625e-05 449.1857586596199\n","[-0.75536938  0.14125184 -0.38940957 -0.39165278] 1.220703125e-05 447.99865498499844\n","[-0.75817767  0.14126407 -0.38730084 -0.39689288] 2.44140625e-05 447.74014107114476\n","[-0.7621022   0.14067206 -0.38454577 -0.40430848] 1.220703125e-05 447.4107870692893\n","[-0.76566228  0.13942582 -0.38226325 -0.41113122] 1.220703125e-05 447.09191402611606\n","[-0.76586722  0.13933638 -0.38213728 -0.41152676] 1.220703125e-05 447.0797746548176\n","[-0.76642931  0.13971568 -0.38161273 -0.41261017] 2.44140625e-05 447.0171162164562\n","[-0.76917338  0.1392415  -0.37972231 -0.4179428 ] 1.220703125e-05 446.7775312629033\n","[-0.78299589  0.13679486 -0.37026295 -0.44564884] 1.220703125e-05 445.55162811120965\n","[-0.7966348   0.13397542 -0.36103373 -0.47445805] 1.220703125e-05 444.30492315745755\n","[-0.80872043  0.13213655 -0.35259421 -0.50131313] 1.220703125e-05 443.18084766960783\n","[-0.80880674  0.13176305 -0.35263548 -0.50151129] 1.220703125e-05 443.1726812122368\n","[-0.81453438  0.13130831 -0.34848916 -0.51470964] 1.220703125e-05 442.65003804366313\n","[-0.81516262  0.13083802 -0.3481531  -0.51617794] 6.103515625e-06 442.56342893911057\n","[-0.82348636  0.129719   -0.34223053 -0.53598414] 1.220703125e-05 441.7889087790376\n","[-0.83930689  0.12678126 -0.33123924 -0.57563858] 1.220703125e-05 440.20655003665394\n","[-0.84267893  0.1256207  -0.32908058 -0.58445441] 1.220703125e-05 439.865160126997\n","[-0.85379455  0.12422791 -0.32146343 -0.6144635 ] 1.220703125e-05 438.717123788758\n","[-0.85975202  0.1229027  -0.31771149 -0.63117976] 1.220703125e-05 438.08938206192414\n","[-0.86398696  0.12216759 -0.31507915 -0.64334004] 1.220703125e-05 437.6470231119447\n","[-0.86870941  0.12170566 -0.31215018 -0.65717779] 6.103515625e-06 437.13619951474953\n","[-0.87553023  0.12073428 -0.30822104 -0.67769957] 6.103515625e-06 436.4017879957408\n","[-0.88618567  0.11955272 -0.3025544  -0.71106245] 1.220703125e-05 435.2437292776768\n","[-0.89289539  0.11853422 -0.29943231 -0.7329176 ] 1.220703125e-05 434.48838460271156\n","[-0.90105479  0.11753234 -0.29593357 -0.76040722] 1.220703125e-05 433.576095910909\n","[-0.90363703  0.11773546 -0.29475538 -0.7693192 ] 1.220703125e-05 433.28034172216167\n","[-0.90578986  0.11759115 -0.29389095 -0.7768306 ] 1.220703125e-05 433.03758816355946\n","[-0.90853293  0.11677451 -0.29300452 -0.78651081] 1.220703125e-05 432.72817678309724\n","[-0.93035259  0.11578504 -0.28547757 -0.86795838] 1.220703125e-05 430.1383776154952\n","[-0.93557361  0.11526631 -0.28398319 -0.88868927] 6.103515625e-06 429.49146015217224\n","[-0.93589821  0.11515039 -0.28391666 -0.88999516] 1.220703125e-05 429.4522858310477\n","[-0.9369106   0.11520302 -0.2835915  -0.894079  ] 6.103515625e-06 429.3278498649511\n","[-0.94087302  0.11518766 -0.28240146 -0.91025094] 1.220703125e-05 428.8511774897262\n","[-0.94162421  0.11487897 -0.28226744 -0.91335198] 6.103515625e-06 428.74634600152706\n","[-0.94440737  0.11479093 -0.28147658 -0.92493293] 6.103515625e-06 428.4006444639273\n","[-0.95129044  0.11431715 -0.27965094 -0.95423786] 2.44140625e-05 427.5384213745051\n","[-0.9561574   0.11393442 -0.27841952 -0.97554679] 1.220703125e-05 426.93178444504713\n","[-0.95906607  0.11409123 -0.27758681 -0.98852048] 1.220703125e-05 426.54549237339415\n","[-0.95934269  0.11399051 -0.27754158 -0.98976428] 1.220703125e-05 426.51054153902714\n","[-0.96348059  0.11377168 -0.27650081 -1.00856623] 2.44140625e-05 425.97765363350567\n","[-0.96748116  0.11374002 -0.27546227 -1.02710842] 6.103515625e-06 425.4547873021162\n","[-0.97450442  0.11325275 -0.27380133 -1.06056189] 1.220703125e-05 424.536652344244\n","[-0.98023183  0.11302097 -0.27243566 -1.08872525] 1.220703125e-05 423.7744136901626\n","[-0.98027909  0.11317702 -0.27237902 -1.08896046] 1.220703125e-05 423.7596678183563\n","[-0.98611043  0.11298645 -0.2710053  -1.11849492] 1.220703125e-05 422.97222138263203\n","[-0.98942958  0.11295088 -0.27021533 -1.13570446] 6.103515625e-06 422.51983626835397\n","[-0.99174366  0.11301127 -0.26964519 -1.14787884] 2.44140625e-05 422.20799519645897\n","[-0.99204618  0.1128644  -0.26961581 -1.14948189] 1.220703125e-05 422.16111865891935\n","[-0.9930776   0.11283347 -0.26938011 -1.15496456] 1.220703125e-05 422.01923185698496\n","[-0.9946932   0.11293293 -0.26896986 -1.16361168] 2.44140625e-05 421.80247068334353\n","[-0.99635751  0.11251279 -0.26870011 -1.17259878] 1.220703125e-05 421.57371826243184\n","[-0.99727072  0.11263451 -0.26845159 -1.17756223] 1.220703125e-05 421.4396244016271\n","[-0.9993212   0.11240062 -0.26804215 -1.18879688] 1.220703125e-05 421.164060418127\n","[-1.00189564  0.11250041 -0.2674193  -1.20307555] 1.220703125e-05 420.79452536129537\n","[-1.01171413  0.11233941 -0.26524749 -1.25939258] 6.103515625e-06 419.40512982892983\n","[-1.01345656  0.11228879 -0.26487594 -1.26970687] 6.103515625e-06 419.1556937643054\n","[-1.01395971  0.1123903  -0.2647355  -1.27270338] 2.44140625e-05 419.0873529076583\n","[-1.01755939  0.11231929 -0.26396796 -1.2943909 ] 2.44140625e-05 418.5696162318523\n","[-1.01832084  0.11218177 -0.2638425  -1.29903523] 6.103515625e-06 418.45531258197485\n","[-1.01964651  0.11198032 -0.26361306 -1.30716859] 1.220703125e-05 418.2676754285219\n","[-1.0219951   0.11223128 -0.26303619 -1.32172585] 2.44140625e-05 417.92630157831195\n","[-1.0231893   0.11202559 -0.26284084 -1.32920402] 1.220703125e-05 417.7479325353964\n","[-1.02517212  0.11214651 -0.26238487 -1.34173226] 1.220703125e-05 417.4607665956437\n","[-1.02548482  0.11187455 -0.26239705 -1.34372225] 1.220703125e-05 417.41606291435124\n","[-1.02847757  0.11194615 -0.26174828 -1.36293982] 6.103515625e-06 416.97195829219055\n","[-1.03641262  0.1117654  -0.26017123 -1.41555439] 2.44140625e-05 415.7949021791311\n","[-1.03743452  0.1119433  -0.25991399 -1.42251132] 1.220703125e-05 415.64269981855915\n","[-1.04086668  0.11189842 -0.25924142 -1.44619548] 1.220703125e-05 415.12652174719926\n","[-1.04283951  0.11180086 -0.25888032 -1.46003462] 6.103515625e-06 414.8265760641383\n","[-1.04378879  0.1116283  -0.25874371 -1.466754  ] 1.220703125e-05 414.68665921503964\n","[-1.04431802  0.1119394  -0.25855093 -1.47051557] 1.220703125e-05 414.60854571649054\n","[-1.04518208  0.11191753 -0.25838896 -1.47668548] 1.220703125e-05 414.4763983989726\n","[-1.04561571  0.11171058 -0.25836436 -1.47979512] 1.220703125e-05 414.40563953987873\n","[-1.04799603  0.11186538 -0.25786052 -1.49700968] 2.44140625e-05 414.0459856406086\n","[-1.05207481  0.11151533 -0.25718567 -1.52711463] 1.220703125e-05 413.4249307801185\n","[-1.05431519  0.11168194 -0.25671881 -1.54398397] 6.103515625e-06 413.0721193533913\n","[-1.05624257  0.11172256 -0.25635063 -1.55869265] 6.103515625e-06 412.7741232051503\n","[-1.05665152  0.11153037 -0.25633056 -1.56183794] 1.220703125e-05 412.71572730194924\n","[-1.06009933  0.11176473 -0.25563553 -1.58868492] 1.220703125e-05 412.1756052465465\n","[-1.06031053  0.11155808 -0.25565661 -1.59035022] 1.220703125e-05 412.1446139460468\n","[-1.0608362   0.11155168 -0.25556401 -1.59450307] 1.220703125e-05 412.0628784882208\n","[-1.06149998  0.11178131 -0.25537905 -1.59976688] 2.44140625e-05 411.9574475731444\n","[-1.06472445  0.11157264 -0.25486721 -1.62567654] 1.220703125e-05 411.45269356047646\n","[-1.06520286  0.11181411 -0.25471433 -1.62956774] 1.220703125e-05 411.377822085279\n","[-1.06747987  0.11171808 -0.25434602 -1.64826686] 6.103515625e-06 411.0158838635829\n","[-1.06756997  0.11171883 -0.25433026 -1.64901278] 6.103515625e-06 411.00165266232193\n","[-1.07239577  0.11173753 -0.25350666 -1.68964769] 1.220703125e-05 410.23602517087755\n","[-1.07572729  0.11165597 -0.2529801  -1.71851281] 1.220703125e-05 409.70552690416923\n","[-1.07603421  0.11190611 -0.25285878 -1.72120556] 1.220703125e-05 409.65909246741484\n","[-1.07614674  0.1117309  -0.25289049 -1.72219536] 1.220703125e-05 409.6366742487615\n","[-1.07824219  0.11164889 -0.25257609 -1.74076144] 1.220703125e-05 409.3032082906769\n","[-1.08261892  0.11196491 -0.25179888 -1.78046431] 1.220703125e-05 408.59668125006743\n","[-1.08530866  0.11184709 -0.25142361 -1.805512  ] 1.220703125e-05 408.15646671166013\n","[-1.08596091  0.11200778 -0.25128014 -1.81166195] 1.220703125e-05 408.05298136530314\n","[-1.08729738  0.11206173 -0.25106669 -1.82436021] 1.220703125e-05 407.8364524703699\n","[-1.08830697  0.11182437 -0.25098613 -1.8340403 ] 1.220703125e-05 407.66888459584186\n","[-1.09065829  0.11209525 -0.25057087 -1.85687658] 1.220703125e-05 407.2840836894994\n","[-1.09128573  0.11207728 -0.2504873  -1.8630417 ] 1.220703125e-05 407.1796790240201\n","[-1.0915413   0.11213733 -0.25043429 -1.86556132] 1.220703125e-05 407.1400157715901\n","[-1.09232288  0.11202459 -0.25035711 -1.8732996 ] 6.103515625e-06 407.0074139038276\n","[-1.09244895  0.11203071 -0.25033784 -1.87455224] 6.103515625e-06 406.9866555573469\n","[-1.09351423  0.11195451 -0.25021227 -1.88518718] 1.220703125e-05 406.8121398893744\n","[-1.09365805  0.11192105 -0.25020205 -1.88663   ] 1.220703125e-05 406.78982020369835\n","[-1.09423531  0.1119411  -0.25011754 -1.89243722] 1.220703125e-05 406.69414228264566\n","[-1.095022    0.11214896 -0.24995226 -1.90039395] 1.220703125e-05 406.5625768058406\n","[-1.09521613  0.11209572 -0.24994133 -1.90236552] 6.103515625e-06 406.5294308939306\n","[-1.09672147  0.11221253 -0.2497085  -1.91775755] 1.220703125e-05 406.2813859501182\n","[-1.09678312  0.11210283 -0.24973151 -1.91839245] 1.220703125e-05 406.26954113514125\n","[-1.0969393   0.11218951 -0.24968647 -1.92000068] 2.44140625e-05 406.2443487275416\n","[-1.0984359   0.11221591 -0.24948522 -1.93551991] 2.44140625e-05 405.99506846744896\n","[-1.09891073  0.1123111  -0.24939762 -1.94048366] 1.220703125e-05 405.9186760504921\n","[-1.0993869   0.11226553 -0.24935026 -1.9454814 ] 1.220703125e-05 405.8370295554539\n","[-1.10026626  0.11238058 -0.24920761 -1.95476251] 1.220703125e-05 405.69397216073816\n","1.220703125e-05 [-0.71760565 -7.94687151 -2.15908823 -7.76569244]\n","[-1.10228481  0.11232995 -0.24897535 -1.97632914]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_6KsBqtiMgiw","colab_type":"text"},"source":["I ran the algorithm four times. Here's the learned weights."]},{"cell_type":"code","metadata":{"id":"r5TzHN51Vwum","colab_type":"code","colab":{}},"source":["# [-1.63963568  0.11244618 -0.25229998 -1.83450268]\n","# [-1.43047609  0.11237203 -0.25146534 -1.86348854]\n","# [-0.90529042  0.11226296 -0.24705779 -2.05801113]\n","# [-1.13011592  0.11231782 -0.24929065 -1.95950121]"],"execution_count":null,"outputs":[]}]}
